\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.2in]{geometry}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1.5ex}{1ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}
\usepackage{amsmath,amsthm,amssymb,MnSymbol}
\setlength{\parindent}{0in}
\renewenvironment{proof}{{\bfseries Proof}}{\\}
\newcommand{\newlinetab}[0]{$\text{ }\hspace{3mm}$}

%--------------------------------------------------
% Intermediate Statistics 705 Cheatsheet for Test 3
%--------------------------------------------------

\begin{document}

\subsection*{Aymptotic (Large Sample) Theory}
A random sequence $A_{n}$ is:
\begin{flalign}
    &(a) \hspace{2mm} o_{p}(1) \text{ if } A_{n} \xrightarrow{p} 0\\
    &(b) \hspace{2mm} o_{p}(B_{n}) \text{ if } A_{n}/B_{n} \xrightarrow{p} 0\\
    &(c) \hspace{2mm} O_{p}(1) \text{ if } \forall \epsilon>0, \exists M :
        \lim_{n\rightarrow \infty} \mathbb{P}(|A_{n}|>M)<\epsilon \\
    &(d) \hspace{2mm} O_{p}(B_{n}) \text{ if } A_{n}/B_{n} = O_{p}(1)
\end{flalign}
If $Y_{n} \rightsquigarrow Y \implies Y_{n}=O_{p}(1)$\\
If $\sqrt{n}(Y_{n}-c)\rightsquigarrow Y$ $\implies$ $Y_{n}=O_{p}(1/\sqrt{n})$

\subsubsection*{Distances Between Distributions}
For distributions $P$ and $Q$ with pdfs $p$ and $q$:
\begin{flalign}
    &(a) \hspace{2mm} V(P,Q) = \sup_{A} |P(A)-Q(A)| \hspace{3mm} \text{\textbf{total variation} distance}\\
    &(b) \hspace{2mm} K(P,Q) = \int p \text{log}(p/q) \hspace{3mm} \text{\textbf{Kullback-Leibler} divergence}\\
    &(c) \hspace{2mm} d_{2}(P,Q) = \int (p-q)^{2} \hspace{3mm} \mathbf{L_{2}} \text{ distance}
\end{flalign}
A model is \textbf{identifiable} if: $\theta_{1} \neq \theta_{2}$ $\implies$ $K(\theta_{1},\theta_{2})>0$.

\subsubsection*{Consistency}
$\hat{\theta}_{n} = T(X^{n})$ is \textbf{consistent} for $\theta$ if $\hat{\theta}_{n} \xrightarrow{p} \theta$
    (ie if $\hat{\theta}_{n} - \theta = o_{p}(1)$).\\
To show consistency, can show: $\text{Bias}^{2}(\hat{\theta}_{n}) + \text{Var}(\hat{\theta}_{n}) \rightarrow 0$.\\
The MLE is consistent under regularity conditions.\\
MLE not consistent when number of params (or support?) grows.

\subsubsection*{Score and Fisher Information}
The \textbf{score function} is $S(\theta) = \frac{\partial}{\partial\theta} l(\theta) = \frac{\partial}{\partial\theta} \sum_{i=1}^{n} \text{log} \hspace{1pt} p(x_{i}|\theta)$.\\
The \textbf{Fisher information} is defined as
\begin{equation}
    I_{n}(\theta) = \mathbb{E}_{\theta} \left[ S(\theta)^{2} \right] = \text{Var}_{\theta} \left[ S(\theta) \right] 
        = -\mathbb{E}_{\theta} \left[ \frac{\partial^{2}}{\partial\theta^{2}} l(\theta) \right]
        %= -\mathbb{E}_{\theta} \left[ \frac{\partial}{\partial\theta} S(\theta) \right]
\end{equation}
and $I_{n}(\theta) = -n\mathbb{E} \left[ \frac{\partial^{2}}{\partial\theta^{2}} \text{log}\hspace{1pt} p(X_{1};\theta) \right] = nI_{1}(\theta)$.\\
The \textbf{observed information} $\hat{I}_{n}(\theta) = -\sum_{i}\frac{\partial^{2}}{\partial\theta^{2}} \text{log}\hspace{1pt}p(X_{i};\theta)$.\\
Vector case: $S(\theta) = \left[ \frac{\partial l(\theta)}{\partial \theta_{i}} \right]_{i=1,\ldots,K}$ \hspace{1mm}
    $I_{ij} = -\mathbb{E}_{\theta}\left[\frac{\partial^{2} l(\theta)}{\partial\theta_{i}\partial\theta_{j}}\right]_{i,j=1,\ldots,K}$

\subsubsection*{Efficiency and Robustness}
For an estimator $\hat{\theta}_{n}(X^{n})$ of $\theta$, where $X^{n} \stackrel{\text{iid}}{\sim} p(x|\theta)$:\\
If $\sqrt{n}(\hat{\theta}_{n} - \theta) \rightsquigarrow \mathcal{N}(0,v^{2})$, then $v^2$ is the \textbf{asymptotic-Var($\hat{\theta}_{n}$)}.\\
    \newlinetab E.g. for $\hat{\theta}_{n} = \overline{X}_{n}$:
        \hspace{1pt} $v^{2} = \sigma^{2} = \text{Var}(X_{i}) = \lim_{n \rightarrow \infty} n\text{Var}(\overline{X}_{n})$.\\
    \newlinetab In general, asymptotic-Var($\hat{\theta}_{n}$) $v^{2}$ $\neq$ $\lim_{n \rightarrow \infty} n\text{Var}(\hat{\theta}_{n})$.\\
    \newlinetab We will use approx: $\text{Var}(\hat{\theta}_{n}) \approx v^{2}/n$.\\
For param $\tau(\theta)$, $v(\theta) = \frac{|\tau'(\theta)|^{2}}{I_{1}(\theta)}$ is the \textbf{Cramer-Rao lower bound}.\\
    \newlinetab since, for most estimators $\hat{\theta}_{n}$, the asymptotic-Var($\hat{\theta}_{n}$) $\geq v(\theta)$.\\
If $\sqrt{n}(\hat{\theta}_{n}-\tau(\theta)) \rightsquigarrow \mathcal{N}(0,v(\theta))$ (ie if $v^{2} = v(\theta)$) $\implies \hat{\theta}_{n}$ \textbf{efficient}.\\
    \newlinetab (usually) $\sqrt{n}(\tau(\hat{\theta}_{\text{mle}}) - \tau(\theta)) \rightsquigarrow \mathcal{N}(0,v(\theta))$ $\implies$ MLE efficient.\\
The \textbf{standard error} of \textbf{efficient} $\hat{\theta}_{n}$ is $se = \sqrt{\text{Var}(\hat{\theta}_{n})} \approx \sqrt{\frac{1}{I_{n}(\theta)}}$.\\
The \textbf{estimated standard error} of \textbf{efficient} $\hat{\theta}_{n}$ is $\hat{se} \approx \sqrt{\frac{1}{I_{n}(\hat{\theta}_{n})}}$.\\
    \newlinetab For efficient $\hat{\theta}_{n}$, $\hat{\tau} = \tau(\hat{\theta}_{n})$, $se \approx \sqrt{\frac{|\tau'(\theta)|^{2}}{I_{n}(\theta)}}$,
        and $\hat{se} \approx \sqrt{\frac{|\tau'(\hat{\theta}_{n})|^{2}}{I_{n}(\hat{\theta}_{n})}}$.\\
In general, \textbf{asymptotic normality} is when:\\
    \newlinetab $\frac{\hat{\theta}_{n} - \mathbb{E}(\hat{\theta}_{n})}{\sqrt{\text{Var}(\hat{\theta}_{n})}} \rightsquigarrow \mathcal{N}(0,1)$
        $\implies$ $\hat{\theta}_{n} \rightsquigarrow \mathcal{N}(\mathbb{E}(\hat{\theta}_{n}), \text{Var}(\hat{\theta}_{n}))$. \\
If $\sqrt{n}(W_{n}-\tau(\theta)) \rightsquigarrow \mathcal{N}(0,\sigma^{2}_{W})$ and
    $\sqrt{n}(V_{n}-\tau(\theta)) \rightsquigarrow \mathcal{N}(0,\sigma^{2}_{V})$ \\
    $\text{ }\hspace{2mm}$$\implies$ \textbf{asymptotic relative efficiency} $\text{ARE}(V_{n},W_{n}) = \sigma^{2}_{W} / \sigma^{2}_{V}$.\\
Often there is a tradeoff between efficiency and robustness. (?)

\newpage
\subsection*{Hypothesis Testing}
\textbf{Null hypothesis} $H_{0}: \theta \in \Theta_{0}$, \textbf{alternative} $H_{1}: \theta \in \Theta_{1}$.\\
\textbf{Type I error}: If $H_{0}$ true but we reject $H_{0}$.\\
To construct a test:
\begin{equation}
    \begin{split}
        &1. \text{ Choose a test statistic } W = W(X_{1},\ldots,X_{n})\\
        &2. \text{ Choose a rejection region } R\\
        &3. \text{ If } W\in R, \text{ reject } H_{0} \text{ otherwise retain } H_{0}
    \end{split}
\end{equation}
For rejection region $R$, the \textbf{power function} $\beta(\theta) = \mathbb{P}_{\theta}(X^{n} \in R)$.\\
Want \textbf{level-$\mathbf{\alpha}$} test ($\sup_{\theta \in \Theta_{0}} \beta(\theta) \leq \alpha$) that maximizes $\beta(\theta_{1})$.\\
A level-$\alpha$ test with power fn $\beta$ is \textbf{uniformly most powerful} if:
    \newlinetab$\beta(\theta) \geq \beta'(\theta)$ $\forall \theta \in \Theta_{1}$ $\forall \beta'\neq\beta$.
\subsubsection*{Neyman-Pearson Test}
For simple $H_{0}: \theta=\theta_{0}$ and $H_{1}: \theta=\theta_{1}$, reject $H_{0}$ if $\frac{L(\theta_{1})}{L(\theta_{0})} > k$.
    \newlinetab where $k$ chosen s.t. $\mathbb{P}(\frac{L(\theta_{1})}{L(\theta_{0})} > k) = \alpha$.
\subsubsection*{Wald Test}
For $H_{0}: \theta=\theta_{0}$ and $H_{1}: \theta\neq\theta_{0}$, reject $H_{0}$ if $\left| \frac{\hat{\theta}_{n} - \theta_{0}}{se} \right| > z_{\alpha/2}$.\\
    \newlinetab where $z_{\alpha/2}$ is the inverse standard-normal CDF of $1-\frac{\alpha}{2}$. \\%, ie $\Phi^{-1}(1-\alpha/2)$
    \newlinetab and $\hat{\theta}_{n}$ is an unbiased estimator for $\theta$.\\
    \newlinetab and $se = \sqrt{\text{Var}(\hat{\theta}_{n})}$. Can also use $\hat{se} =_{\text{eg.}} \sqrt{S_{n}^{2}/n}$.\\
    \newlinetab and if $\hat{\theta}_{n}$ efficient, can approx: $se \approx \sqrt{\frac{1}{I_{n}(\theta)}}$ or $\hat{se} \approx \sqrt{\frac{1}{I_{n}(\hat{\theta}_{n})}}$.

\subsubsection*{Likelihood Ratio Test}
For $H_{0}: \theta\in\Theta_{0}$ and $H_{1}: \theta\notin\Theta_{0}$, reject $H_{0}$ if $\lambda(x^{n}) = \frac{L(\hat{\theta}_{0})}{L(\hat{\theta})} \leq c$.
    \newlinetab where $L(\hat{\theta}_{0}) = \sup_{\theta\in\Theta_{0}}L(\theta)$ and $L(\hat{\theta}) = \sup_{\theta\in\Theta}L(\theta)$.\\
    \newlinetab and $c$ chosen s.t. $\mathbb{P}(\lambda(x^{n}) \leq c) = \alpha$.\\
    \newlinetab \textbf{Thm:} under $H_{0} : \theta=\theta_{0}$ $\implies$ $W_{n} = -2\text{log}\lambda(X^{n}) \rightsquigarrow \chi_{1}^{2}$\\
        \newlinetab\newlinetab $\implies \lim_{n\rightarrow\infty} \mathbb{P}_{\theta_{0}}(W_{n} > \chi_{1,\alpha}^{2}) = \alpha$.\\
    \newlinetab\newlinetab Also: for $\theta=(\theta_{1},\ldots,\theta_{k})$, if $H_{0}$ fixes some of the parameters\\
    \newlinetab\newlinetab $\implies$ $-2\text{log}\lambda(X^{n}) \rightsquigarrow \chi_{\nu}^{2}$, where $\nu = \text{dim}(\Theta) - \text{dim}(\Theta_{0})$.

\subsubsection*{P-Values}
The \textbf{p-value} $p(x^{n})$ is the smallest $\alpha$-level s.t. we reject $H_{0}$.\\
\textbf{Thm:} For a test of the form: reject $H_{0}$ when $W(x^{n})>c$,\\
    $\text{ }\hspace{1mm}$ $\implies$ $p(x^{n}) = \underset{\theta\in\Theta_{0}}{\sup} \mathbb{P}_{\theta}(W(X^{n}) \geq W(x^{n}))$
    $=$ $\underset{\theta\in\Theta_{0}}{\sup} [1 - F(W(x^{n})|\theta)]$.\\
\textbf{Thm:} Under $H_{0}:\theta=\theta_{0}$,\hspace{2mm}$p(x^{n}) \sim \text{Unif}(0,1)$.

\subsubsection*{Permutation Test}

\end{document}
